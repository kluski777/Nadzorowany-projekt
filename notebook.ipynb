{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml  # noqa: F401 (import comet_ml before pytorch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COMET_API_KEY\"] = \"QKss0LkkOPjmHkPH4B2koQS16\"\n",
    "os.environ[\"COMET_PROJECT_NAME\"] = \"unsupervised-learning\"\n",
    "os.environ[\"COMET_WORKSPACE\"] = \"ekipa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb46d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"num_workers\": 3,\n",
    "        \"image_size\": 256,\n",
    "        \"total_samples\": 2048,\n",
    "        \"val_split\": 0.1,\n",
    "        \"test_split\": 0.1,\n",
    "        \"data_dir\": \"/kaggle/working/data\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_channels\": 3,\n",
    "        \"latent_channels\": 128,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"max_epochs\": 150,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"early_stopping_patience\": 10,\n",
    "        \"lr_scheduler_patience\": 5,\n",
    "        \"lr_scheduler_factor\": 0.5,\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"name\": \"autoencoder-d-7-5s\",\n",
    "        \"seed\": 42,\n",
    "        \"visualization_samples\": 8,\n",
    "        \"recon_log_every_n_epochs\": 5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43256063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        latent_channels: int = 128,\n",
    "        learning_rate: float = 1e-3,\n",
    "        scheduler_patience: int = 5,\n",
    "        scheduler_factor: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scheduler_patience = scheduler_patience\n",
    "        self.scheduler_factor = scheduler_factor\n",
    "\n",
    "        # U-NET https://arxiv.org/pdf/1505.04597\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (input_channels x 256 x 256) -> (64 x 128 x 128)\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (64 x 128 x 128) -> (128 x 64 x 64)\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (128 x 64 x 64) -> (256 x 32 x 32)\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (256 x 32 x 32) -> (512 x 16 x 16)\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (512 x 16 x 16) -> (latent_channels x 8 x 8)\n",
    "            nn.Conv2d(512, latent_channels, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(latent_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (latent_channels x 8 x 8) -> (512 x 16 x 16)\n",
    "            nn.ConvTranspose2d(\n",
    "                latent_channels, 512, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (512 x 16 x 16) -> (256 x 32 x 32)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (256 x 32 x 32) -> (128 x 64 x 64)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "\n",
    "            # (128 x 64 x 64) -> (64 x 128 x 128)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            # (64 x 128 x 128) -> (3 x 256 x 256)\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        latent_space = self.encoder(x)\n",
    "        reconstructed_image = self.decoder(latent_space)\n",
    "        return reconstructed_image\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        reconstructed = self(images)\n",
    "\n",
    "        loss = nn.functional.mse_loss(reconstructed, images)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        reconstructed = self(images)\n",
    "\n",
    "        loss = nn.functional.mse_loss(reconstructed, images)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=self.scheduler_factor,\n",
    "            patience=self.scheduler_patience,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ba381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtStreamingDataset(IterableDataset):\n",
    "    \"\"\"Streaming Dataset wrapper for WikiArt that applies transforms.\"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.dataset:\n",
    "            image = item[\"image\"]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            yield {\"image\": image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395996a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for WikiArt dataset with local disk caching.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 0,\n",
    "        image_size: int = 256,\n",
    "        total_samples: Optional[int] = None,\n",
    "        val_split: float = 0.1,\n",
    "        test_split: float = 0.1,\n",
    "        data_dir: str = \"/kaggle/working/data\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = image_size\n",
    "        self.total_samples = total_samples\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        self.data_dir = Path(data_dir)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download dataset to local directory if not already downloaded.\"\"\"\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        dataset_path = self.data_dir / \"Artificio___WikiArt_Full\"\n",
    "        if not dataset_path.exists():\n",
    "            print(f\"Downloading WikiArt dataset to {self.data_dir}...\")\n",
    "            load_dataset(\n",
    "                \"Artificio/WikiArt_Full\",\n",
    "                cache_dir=str(self.data_dir),\n",
    "                keep_in_memory=False,\n",
    "            )\n",
    "            print(\"Dataset download completed!\")\n",
    "        else:\n",
    "            print(f\"Dataset already exists at {dataset_path}\")\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load dataset from local disk using streaming.\"\"\"\n",
    "        \n",
    "        dataset = load_dataset(\n",
    "            \"Artificio/WikiArt_Full\",\n",
    "            cache_dir=str(self.data_dir),\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        if self.total_samples is None:\n",
    "            self.total_samples = 103_250\n",
    "\n",
    "        self.test_size = int(self.total_samples * self.test_split)\n",
    "        self.val_size = int(self.total_samples * self.val_split)\n",
    "        self.train_size = self.total_samples - self.val_size - self.test_size\n",
    "\n",
    "        train_hf = dataset.take(self.train_size)\n",
    "        val_hf = dataset.skip(self.train_size).take(self.val_size)\n",
    "        test_hf = dataset.skip(self.train_size + self.val_size).take(self.test_size)\n",
    "\n",
    "        self.train_dataset = WikiArtStreamingDataset(train_hf, transform=self.transform)\n",
    "        self.val_dataset = WikiArtStreamingDataset(val_hf, transform=self.transform)\n",
    "        self.test_dataset = WikiArtStreamingDataset(test_hf, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionLogger(Callback):\n",
    "    \"\"\"Logs reconstruction examples to Comet every N epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, log_every_n_epochs: int = 5, num_samples: int = 8):\n",
    "        super().__init__()\n",
    "        self.log_every_n_epochs = log_every_n_epochs\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_batch = None\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx == 0 and self.sample_batch is None:\n",
    "            self.sample_batch = {\n",
    "                \"image\": batch[\"image\"][: self.num_samples].detach().cpu()\n",
    "            }\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if self.sample_batch is None:\n",
    "            return\n",
    "\n",
    "        if (trainer.current_epoch + 1) % self.log_every_n_epochs != 0:\n",
    "            return\n",
    "\n",
    "        images = self.sample_batch[\"image\"].to(pl_module.device)\n",
    "        pl_module.eval()\n",
    "        with torch.inference_mode():\n",
    "            reconstructed = pl_module(images)\n",
    "            if isinstance(reconstructed, tuple):\n",
    "                reconstructed = reconstructed[0]\n",
    "\n",
    "        pl_module.train()\n",
    "        images = images.cpu()\n",
    "        reconstructed = reconstructed.cpu()\n",
    "        comparison = torch.stack([images, reconstructed], dim=1)\n",
    "        comparison = comparison.view(-1, *images.shape[1:])\n",
    "        grid = vutils.make_grid(\n",
    "            comparison,\n",
    "            nrow=2,\n",
    "            normalize=True,\n",
    "            value_range=(0, 1),\n",
    "            padding=2,\n",
    "        )\n",
    "        if trainer.logger is not None:\n",
    "            trainer.logger.experiment.log_image(\n",
    "                grid.permute(1, 2, 0).numpy(),\n",
    "                name=\"reconstructions\",\n",
    "                step=trainer.current_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(\n",
    "    model,\n",
    "    data_module,\n",
    "    num_samples: int = 8,\n",
    "    output_path: str = \"reconstruction_results.png\",\n",
    "):\n",
    "    model.eval()\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    batch = next(iter(val_loader))\n",
    "    images = batch[\"image\"][:num_samples]\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        reconstructed = model(images)\n",
    "        if isinstance(reconstructed, tuple):\n",
    "            reconstructed = reconstructed[0]\n",
    "\n",
    "    images = images.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "    _, axes = plt.subplots(2, num_samples, figsize=(20, 5))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img_orig = images[i].permute(1, 2, 0).numpy()\n",
    "        axes[0, i].imshow(np.clip(img_orig, 0, 1))\n",
    "        axes[0, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title(\"Original\", fontsize=12)\n",
    "        img_recon = reconstructed[i].permute(1, 2, 0).numpy()\n",
    "        axes[1, i].imshow(np.clip(img_recon, 0, 1))\n",
    "        axes[1, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title(\"Reconstructed\", fontsize=12)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"\\nResults saved to '{output_path}'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(CONFIG[\"experiment\"][\"seed\"], workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bac34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = WikiArtDataModule(\n",
    "    batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "    num_workers=CONFIG[\"data\"][\"num_workers\"],\n",
    "    image_size=CONFIG[\"data\"][\"image_size\"],\n",
    "    total_samples=CONFIG[\"data\"][\"total_samples\"],\n",
    "    val_split=CONFIG[\"data\"][\"val_split\"],\n",
    "    test_split=CONFIG[\"data\"][\"test_split\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c35d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    input_channels=CONFIG[\"model\"][\"input_channels\"],\n",
    "    latent_channels=CONFIG[\"model\"][\"latent_channels\"],\n",
    "    learning_rate=CONFIG[\"model\"][\"learning_rate\"],\n",
    "    scheduler_patience=CONFIG[\"training\"][\"lr_scheduler_patience\"],\n",
    "    scheduler_factor=CONFIG[\"training\"][\"lr_scheduler_factor\"],\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84144092",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"autoencoder-{epoch:02d}-{val_loss:.4f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=CONFIG[\"training\"][\"early_stopping_patience\"],\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "logger = CometLogger(\n",
    "    api_key=os.environ[\"COMET_API_KEY\"],\n",
    "    project=os.environ[\"COMET_PROJECT_NAME\"],\n",
    "    workspace=os.environ[\"COMET_WORKSPACE\"],\n",
    "    name=CONFIG[\"experiment\"][\"name\"],\n",
    ")\n",
    "\n",
    "recon_logger = ReconstructionLogger(\n",
    "    log_every_n_epochs=CONFIG[\"experiment\"][\"recon_log_every_n_epochs\"],\n",
    "    num_samples=CONFIG[\"experiment\"][\"visualization_samples\"],\n",
    ")\n",
    "\n",
    "logger.log_hyperparams(CONFIG)\n",
    "logger.experiment.log_parameter(\"config_source\", \"notebook_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e036bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG[\"training\"][\"max_epochs\"],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, recon_logger],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=CONFIG[\"training\"][\"gradient_clip_val\"],\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)\n",
    "print(f\"Best model path: {checkpoint_callback.best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953914c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(\n",
    "    model, data_module, num_samples=CONFIG[\"experiment\"][\"visualization_samples\"]\n",
    ")\n",
    "logger.experiment.log_image(\"reconstruction_results.png\", name=\"Final Reconstructions\")\n",
    "logger.experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
